import sys
import json

sys.path.append('./')
from agent_adviser.serving.search_pipeline import search_api
from agent_adviser.configs.model_config import RAG_PROMPT


def build_llm_prompt(question: str, rerank_results: list):
    """
    :param rerank_results: ES搜索并排序后的结果，具体数据结构如下：
        [
            {
                "_index": "papers_search_index",
                "_id": "b9990ceab0dc4f6de255c557b7baff47",
                "_score": 0.5502805,
                "_ignored": [
                    "references.keyword"
                ],
                "_source": {
                    "item_id": "b9990ceab0dc4f6de255c557b7baff47",
                    "title": "EnvGen- Generating and Adapting Environments via LLMs for Training Embodied Agents",
                    "authors": {
                        "authors": [
                            {
                                "name": "Abhay Zala",
                                "work": "UNC Chapel Hill",
                                "contact": "aszala@cs.unc.edu"
                            },
                            {
                                "name": "Jaemin Cho",
                                "work": "UNC Chapel Hill",
                                "contact": "jmincho@cs.unc.edu"
                            },
                            {
                                "name": "Han Lin",
                                "work": "UNC Chapel Hill",
                                "contact": "hanlincs@cs.unc.edu"
                            },
                            {
                                "name": "Jaehong Yoon",
                                "work": "UNC Chapel Hill",
                                "contact": "jhyoon@cs.unc.edu"
                            },
                            {
                                "name": "Mohit Bansal",
                                "work": "UNC Chapel Hill",
                                "contact": "mbansal@cs.unc.edu"
                            }
                        ]
                    },
                    "date": "2024-03-18",
                    "abstract": "Recent state-of-the-art approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. This begs an interesting question: Instead of directly employing LLMs as embodied agents, can we use LLMs’ reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? In this work, we propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and environment simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g., different terrains, items initially given to agents, chance of finding certain objects, etc.). Next, we train a small RL agent in a mixture of the original and LLM-generated environments. Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent’s performance. We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist game environments. We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster. We also show qualitatively how the LLM adapts training environments to help improve RL agents’ weaker skills over time. Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require one or more LLM calls per step (resulting in thousands of LLM calls per episode). Lastly, we present detailed ablation studies for EnvGen’s design choices.",
                    "body": "\n1\nIntroduction\nThere has been growing interest in embodied AI, where agents learn through interactions\nwith environments instead of static datasets (Ahn et al., 2022; Duan et al., 2022; Wang et al.,\n2023a; Yao et al., 2023; Driess et al., 2023). Open-world games such as Minecraft (Mojang\nStudios, 2009) and Crafter (Hafner, 2022) have been widely used as research environments\nfor embodied agents, where the agents visually perceive their surroundings, traverse large\nterrains, and learn to unlock various achievements (e.g., collecting resources, building tools,\ndefeating monsters, etc.). Some achievements can be easily unlocked within a few steps,\nwhereas others are more challenging as they only become accessible after the agent com-\npletes a series of prerequisite achievements, requiring hundreds of steps (i.e., long-horizon\ntasks). As illustrated in Fig. 1 (a), traditional embodied agents are based on reinforcement\n∗equal contribution\n1\narXiv:2403.12014v1  [cs.CL]  18 Mar 2024\n (a) RL agents: Explore skills with rewards\n(b) LLM agents: Explore skills with LLM knowledge\n(c) EnvGen (Ours): Bootstrap skill exploration with LLM-generated environments\nLLM\nAgent\nInteract\nInteract\nEnvironment\nSmall RL\nAgent\nEnvironment\n Custom Environments\n…\nStep 1: Generate \ntraining environments\nLLM\nOriginal\nEnvironment\nSmall RL\nAgent\nStep 2: Train small agent in\ngenerated environments\nStep 3: Train and measure \nagent performance in the original environment\nStep 4: Share agent \nperformance with LLM\nFeedback to \nupdate \nenvironments\nLoop NCycle times\nFigure 1: Comparison of different methods for creating embodied agents. Previous works commonly\nuse (a) small RL agents or (b) LLM agents to explore skills. In (c) EnvGen, we train a small RL agent\nwith diverse LLM-generated environments that train different skills in parallel and can be adapted\nvia feedback to help the agents progressively improve skills that they are weaker at. Our method\nbenefits from using the world knowledge from LLMs while maintaining efficient training through a\nlightweight RL agent.\nlearning (RL) (Hafner et al., 2020; 2021; 2023; Schulman et al., 2017; Burda et al., 2018; Hessel\net al., 2018; Sekar et al., 2020; Moon et al., 2023). However, these RL agents usually struggle\nwhen learning such long-horizon tasks since the reward is sparsely given only after the\ncorrect execution of successive actions, and it is very expensive to automatically find many\naction sequences which lead to the reward (Aytar et al., 2018; Li et al., 2022a; Yuan et al.,\n2023), even after long pretraining with curiosity-driven intrinsic reward (Walker et al., 2023).\nAs large language models (LLMs) have shown remarkable progress in various tasks that\nrequire complex reasoning (Brown et al., 2020; OpenAI, 2023a; Touvron et al., 2023a;b;\nChowdhery et al., 2023; Anil et al., 2023), recent works study implementing embodied\nagents based on LLMs. As illustrated in Fig. 1 (b), these methods leverage LLMs’ world\nknowledge with chain-of-thought reasoning (Nye et al., 2021; Kojima et al., 2022; Wei et al.,\n2022) by creating action plans, giving feedback, and obtaining rewards throughout the\nepisode (Yuan et al., 2023; Wang et al., 2023c; Wu et al., 2023; Wang et al., 2023a;d; Zhao\net al., 2023; Du et al., 2023). While these LLM-based agents that verbalize their knowledge in\nreasoning steps have seen success in achieving better performance over previous approaches,\niteratively calling LLMs throughout the episode is prohibitively slow and expensive (e.g.,\nSPRING (Wu et al., 2023) calls GPT-4 (OpenAI, 2023a) 9 times to take any action step, which\nresults in $270 USD to complete an episode). Du et al. (2023) use LLMs to create rewards to\ntrain smaller agents, but the training is still costly, as it requires many interactions between\nthe LLMs and student agents. This begs the question: Instead of directly employing LLMs as\nembodied agents, can we use LLMs’ reasoning capability to adaptively create training environments\nto help smaller embodied RL agents learn useful skills that they are weak at?\nTo address this question, we propose EnvGen, a novel framework where an LLM adaptively\ngenerates training environments to teach smaller embodied RL agents. We aim to generate\nenvironments that can create various conditions (e.g., have different terrains or some sub-\ngoals are already achieved) so that agents can learn different skills in parallel and obtain\nmore frequent rewards for challenging long-horizon tasks than in the original environment.\nAs shown in Fig. 1 (c), EnvGen iterates over multiple training cycles, each consisting of the\nfollowing four steps.\n• Step 1: We generate configurations for custom training environments (i.e., specifically\ncreated to train an RL agent on certain skills) by providing an LLM with a prompt\nincluding task description, controllable simulator settings, and simulator constraints (see\nFig. 2 and Sec. 2 for details). Then we use the generated configurations to create different\ncustom environments (e.g., different terrains, items initially given to agents, and chance\nof finding certain objects) that can teach multiple skills in parallel.\n• Step 2: We train the RL agent in multiple LLM-generated environments (i.e., LLM envi-\nronments), so that it can learn different useful skills in parallel.\n• Step 3: We first train the RL agent in the original environment to mitigate overfitting to\nthe LLM environments. Then we measure the current RL agent’s performance in different\ntasks in the original environment to check which skills/tasks the agent is still weak at.\n2\n • Step 4: We provide the RL agent’s successes/failures in different tasks (from step 3) as\nfeedback to the LLM, so that the LLM can adapt the custom training environments to\nfocus on progressively improving the skills that the agent is weak at.\nNote that EnvGen only requires a few LLM calls (e.g., 4) for environment genera-\ntion/updating during the entire RL agent training process, whereas other works based on\nLLM agents query an LLM once or multiple times every step (resulting in thousands of\nLLM calls for a single episode).\nWe study the usefulness of EnvGen in different game environments: Crafter (Hafner, 2022)\nand Heist (Cobbe et al., 2020). In the Crafter environment, a simple PPO-based (Schulman\net al., 2017) lightweight (< 5M parameters) RL agent trained with our LLM-generated\nenvironments outperforms strong baselines including a GPT-4 based agent that queries\nan LLM multiple times at every step, and RL agents that use extensive pretraining (e.g.,\n150M steps vs. less than 1M steps for us). When compared to just training longer in\nthe original Crafter environment, an RL agent trained with EnvGen achieves significant\nimprovements on the overall score and long-horizon tasks. In Heist, we also show that\nour LLM-generated environments can improve overall agent performance and training\nstability. We also show a qualitative study on how the LLM adapts training environments\nto help improve RL agents’ weaker skills over time. Finally, we provide comprehensive\nanalysis and ablation studies of the design choices of EnvGen, including EnvGen vs. longer\ntraining in the original environment, adaptively updating LLM-generated environments vs.\na fixed environment, different LLMs for generating environments, frequency of environment\nupdates, the number of LLM-generated environments, and the mixture ratio between the\noriginal and LLM environment during training.\n2\nEnvGen: Generating and Adapting Environments via LLMs for\nTraining Embodied Agents\nWe propose EnvGen, a novel framework where an LLM adaptively generates training\nenvironments to train smaller embodied RL agents, enabling them to accomplish various\ntasks within an environment, particularly long-horizon tasks. During the training process,\nthe LLM is given feedback (in the form of the agent’s performance) and can adaptively\nupdate the training environments to progressively focus on improving the tasks that the\nagent is weak at. In the following, we first explain why it is challenging to explore long-\nhorizon tasks in open-world games (Sec. 2.1). Then we explain our method details, including\nhow we generate environments and how agents are trained in the generated and original\nenvironments (Sec. 2.2).\n2.1\nPreliminary: Exploration is Hard for Long-Horizon Tasks\nIn the RL framework, agents explore various states along a trajectory and amplify policies\nbased on the rewards received from those trajectories. However, exploration for long-\nhorizon tasks is slow and computationally expensive, as rewards for such tasks are sparsely\ngiven only after a sequence of successful actions that often involve achieving multiple\nsubgoals. For example, the goal in Crafter (Hafner, 2022) is to unlock 22 achievements,\nwhere some achievements can be unlocked quickly through several simple actions and\nothers require long chains of prerequisites (e.g., collect iron requires make stone pickaxe, which\nmust be preceded by collect stone, ... etc.); see Sec. 3.1 for details. As shown in Hafner (2022),\nexisting agents in Crafter spend most exploration steps learning low-level achievements but\nfail to unlock high-order achievements with many prerequisites.\n2.2\nEnvGen Method Details\nWe introduce EnvGen, where we train an embodied RL agent in multiple LLM-generated\nenvironments (we refer to these as ‘LLM environments’ in the paper) that progressively\nadapt to improve agent performance in multiple skills. The generated environments can\nprovide various conditions (e.g., different terrains, or some subgoals are already achieved)\n3\n Custom Env 1\nCustom Env 2\nCustom Env N\n…\nLLM\nEnv 1: Basic Resource \nCollection and Crafting\n```json\n{\n    \"environment_settings\": {\n        \"target_biome\": \"grassland\",\n        \"tree_rarity\": \"common\",\n        \"coal_rarity\": \"common\"\n    },\n    \"inventory_settings\": {\n        \"wood\": 3,\n        \"stone\": 0,\n        \"wood_pickaxe\": 1\n    }\n}\n```\nPurpose: \nThe agent can \nlearn to collect \nwood and craft \nitems like a wood \nsword or a table ...\n# Game and task description\nYou are an environment designer \nagent for a game called \"Crafter\". Your \njob is to design a few environments \nwhich can be used to teach an agent \nhow to play...\n# Game Objectives\nHere is a list of things an agent would \nneed to learn how to do:\n● collect_coal, collect_diamond ...\n# Controllable Simulator Settings\nHere is a list of parameters you can \ncontrol when making an environment:\n● target_biome: grassland | mountain \n| beaches | natural \n● ...\nHere is a list of items the agent can \nstart with:\n● wood_pickaxe: 0-1 \n● …\n(Continued)\n# Simulator Constraints\nHere is a list of constraints:\n● natural biome will set the \nenvironment to have all the \nbiomes\n● coal, iron, and diamond can only \nbe found in a mountain biome ...\n# Output Template\nOutput in the following format:\nEnvironment 1:\n```json\n{ \n   \"environment_settings\": {...},\n   \"inventory_settings\": {...} \n}\n```\nPurpose:  ...\nEnvironment 2: \n...\nEnv 2: Advanced Resource \nCollection and Combat Training\n```json\n{\n    \"environment_settings\": {\n        \"target_biome\": \"mountain\",\n        \"coal_rarity\": \"common\",\n        \"iron_rarity\": \"common\",\n        \"diamond_rarity\": \"rare\"\n    },\n    \"inventory_settings\": {\n        \"wood\": 5,\n        \"stone\": 3,\n        \"wood_pickaxe\": 1,\n        \"stone_pickaxe\": 1,\n        \"wood_sword\": 1\n    }\n}\n```\nPurpose: ...\nEnv 3: Survival and Crafting \nMastery\n```json\n{\n    \"environment_settings\": {\n        \"target_biome\": \"natural\",\n        \"tree_rarity\": \"common\",\n        \"coal_rarity\": \"common\",\n        \"iron_rarity\": \"common\",\n        \"diamond_rarity\": \"rare\"\n    },\n    \"inventory_settings\": {\n        \"sapling\": 2,\n        \"wood\": 0,\n        \"stone\": 0,\n        \"coal\": 0,\n        \"iron\": 0,\n        \"diamond\": 0\n    }\n}\n```\nPurpose: ...\nSmall RL\nAgent\nOriginal\nEnvironment\nThose environments resulted in the agent improving up to these scores: \n● collect_coal: 38% +/- 6%\n● defeat_skeleton: 10% +/- 4%\n● make_stone_pickaxe: 31% +/- 3%\n● …\nCould you generate new environments based on these scores?\nStep 1: Generate\ntraining environments\nStep 2: Train small agent \nin generated environments\nStep 3: \nTrain and measure agent \nperformance in the original environment\nStep 4: \nShare agent \nperformance with LLM\nFeedback to update environments \nCrafterEnvGe\nn\nLoop NCycle times\nFigure 2: In EnvGen framework, we generate multiple environments with an LLM to let the agent\nlearn different skills effectively, with the NCycle training cycles, each consisting of the following four\nsteps. Step 1: we provide an LLM with a prompt composed of four components (i.e., task description,\nenvironment details, output template, and feedback from the previous cycle), and ask the LLM to\nfill the template and output various environment configurations that can be used to train agents on\ndifferent skills. Step 2: we train a small RL agent in the LLM-generated environments. Step 3: we\ntrain the agent in the original environment to allow for better generalization and then measure the\nRL agent’s training progress by letting it explore the original environment. Step 4: we provide the\nLLM with the agent performance from the original environment (measured in step 3) as feedback for\nadapting the LLM environments in the next cycle to focus on the weaker performing skills.\nso that agents can learn different skills in parallel and obtain more frequent rewards for long-\nhorizon tasks. As shown in Fig. 2, EnvGen iterates NCycle training cycles, each consisting of\nthe following four steps:\nStep 1: Generate training environments with an LLM.\nAs illustrated in step 1 of Fig. 2,\nwe use an LLM (e.g., GPT-4 (OpenAI, 2023a)1) to first generate NLLM-Env custom training\nenvironment configurations2 that can cover various objectives and skills that are required in\nthe original environment. The following describes the LLM input prompt components used\nto create environment configurations.\n1. Task description: We provide a brief description of the environment and what the LLM\nshould do (e.g., “generate a set of training environments...”).\n2. Game/simulator details: We provide a list of objectives that need to be achieved in the\nenvironment (e.g., “collect coal, collect iron, etc.” for Crafter); a list of which simulator\nsettings can be controlled (e.g., terrain, agent inventory); and a list of constraints/rules\nthat the simulator has (e.g., “skeletons only spawn in mountains; cows only spawn in grass; ...”\nfor Crafter).\n3. Output environment configuration template: We provide a blank output configuration\ntemplate (i.e., a JSON object where the environment settings are empty) to the LLM, and\nrequest it to fill in the values, creating NLLM-Env environment configurations. Along\nwith filling the templates, we also ask the LLM to verbally explain the purpose for each\nenvironment (e.g., what the environment would teach the agent); this would help users\neasily understand the environment generation process.\n1We use GPT-4-1106-Preview (i.e., GPT-4 Turbo).\n2We find that N=4 works well; see Table 7 for details.\n4\n 4. Adaptation feedback based on the RL agent’s performance: We provide the LLM with\nthe performance of the RL agent from the original environment (measured in step 3 and\nsummarized in step 4), as feedback for adapting LLM environments to focus on skills\nthat the RL agent is weak at. The feedback is given at the end of each cycle, so it is only\nprovided to LLM from the second cycle onwards.\nThe obtained environment configurations are then rendered in the game’s simulator. Fig. 2\npresents the summary of input prompt and output environments from the GPT-4 model.\nWe provide more prompt details in the appendix.\nStep 2: Train a small RL agent in the LLM-generated environments.\nAs shown in step 2\nof Fig. 2, we first train the small RL agent in the LLM-generated environments. Concretely,\nwe train the agent in the NLLM-Env LLM environments for TLLM-Env total steps in parallel.\nStep 3: Train and measure the RL agent’s performance in the original environment.\nIt\nis important to note that the goal of EnvGen is to improve the RL agent’s performance\nin the original environment, instead of the performance only in the LLM environments.\nTo help the RL agent effectively adapt to the original environment and provide the LLM\nwith the current agent’s performance as feedback, we train the agent and measure its\nperformance in the original environment, as shown in step 3 of Fig. 2. First, to mitigate\nthe overfitting to LLM environments, we train the agent in the original environment for\nTOrig-Env steps.3 Next, to find the skills that the RL agent needs to improve at, we test\nthe agent in the original environment, without any parameter updates. Concretely, we\nmeasure individual success rates for each environment task (e.g., Crafter achievements).\nThe agent performance is summarized (in step 4) and is provided to LLM as feedback (in\nstep 1) to adapt training environments in the next cycle. Moreover, importantly, to obtain a\nmore calibrated estimation of agent performance, we calculate the average and variance\nof the task-specific scores by testing agents with multiple random seeds (i.e., 12). Note\nthis performance measuring takes minimal time and computation compared to the actual\ntraining steps (e.g., 30x faster in our experiments) as it does not involve backpropagation.\nStep 4: Send feedback to LLM to adapt environments (to focus on weak skills).\nWe\nprovide the LLM with the agent’s performance from the original environment (measured in\nstep 3), as feedback for updating LLM environments. Concretely, we list the agent’s average\ntask-specific success rate in percentages along with one standard deviation (e.g., “. . . collect\ncoal: 38% ± 6%, defeat skeleton: 10% ± 4% . . . ”), as shown in step 4 of Fig. 2. In step 1 of\nthe next cycle, the LLM can adaptively generate new environments (by using the agent’s\nperformance as feedback) to better help the RL agent learn the skills it is weak at (e.g., defeat\nskeleton). EnvGen iterates this four-step training cycle NCycle times.\n3\nExperimental Setup\nIn the following subsections, we present the benchmarks in which we evaluate EnvGen\nframework on (Sec. 3.1) and the agent architectures that we use for experiments (Sec. 3.2).\n3.1\nEvaluated Benchmarks and Training Details\nCrafter.\nCrafter (Hafner, 2022) is an open-world 2D survival game focused on evaluating\na broad range of agent capabilities (see Fig. 3). Crafter features 22 achievements that an\nagent can unlock during an episode of play. Some achievements can be unlocked in a\nfew steps (e.g., collect wood, collect sapling, etc.), but other achievements, such as make iron\npickaxe or collect diamond, require many training/exploration steps and several prerequisite\nachievements to be unlocked (see Fig. 3 b). For example, to make a stone pickaxe, an agent\nmust first collect enough wood to make a table and a wooden pickaxe, then go collect stone\nand return to the table (or collect more wood to make a new one) and then construct the\n3We find that TLLM-Env = TOrig-Env works well; see Table 8 for details.\n5\n (a) Agent View\n(b) Achievement Hierarchy\nCollect Sapling\nDefeat Zombie\nDefeat\nSkeleton\nEat Cow\nEat Plant\nPlace Plant\nCollect Wood\nPlace Table\nMake Wood \nSword\nMake Wood \nPickaxe\nMake Stone \nSword\nPlace Stone\nMake Stone \nPickaxe\nPlace Furnace\nCollect Iron\nMake Iron\nPickaxe\nMake Iron\nSword\nCollect Coal\nCollect Stone\nCollect\nDiamond\nCollect Drink\nWake Up\nFigure 3: (a): Crafter gameplay screenshot. An agent explores a 2D world and completes 22 achieve-\nments. (b): Crafter achievement hierarchy. Some achievements can be completed right away; others\nrequire previous achievements to be unlocked first (i.e., in a hierarchical order following the arrows).\nstone pickaxe. As an agent progresses, these prerequisite achievements become increasingly\ncompounded (e.g., an iron pickaxe requires a table + a stone pickaxe + a furnace + some coal\n+ some wood), making it incredibly difficult to complete all achievements. The score for\nCrafter is computed as the geometric mean of individual success rates of each achievement\nfor each episode it is completed within 1M training steps: S = exp( 1\n22 ∑22\ni=1 ln(1 + si)) − 1,\nwhere si is the average success rate of the ith achievement across all episodes that occurred\nduring training.\nFor EnvGen setup, we use NCycle = 4 training cycles during agent training (see Table 6 for\nablation of having a different number of cycles). Each cycle uses 0.12M LLM-generated\nenvironment steps (i.e., CrafterEnvGen steps, see step 2 in Fig. 2) and 0.12M Crafter steps\n(step 3 in Fig. 2) and then we train for 1M steps in Crafter. In total, we train for 1.96M\nsteps ((0.12M + 0.12M) × 4 + 1M). Note that in order to maintain a fair score comparison to\nbaselines, we do not count any achievement during our training cycle for score calculation\nsince the training scores derived from LLM environments and the original environment are\nnot directly comparable. Instead, we only take into account the achievements from the last\n1M training steps in Crafter for the score calculation. We also experiment with giving the\nbaseline model additional original environment steps to match the number of EnvGen steps\n(i.e., an additional 0.96M steps) to ensure that EnvGen is not better simply because of more\nsteps. We report the average performance with 30 runs (= 3 different initial LLM-generated\nCrafterEnvGen environments × 10 different random seeds).\n(a) Agent View\n(b) Achievement Hierarchy\nOpen Blue Lock\nOpen Green Lock\nOpen Red Lock\nSteal Gem\nFigure 4: (a): Heist gameplay screenshot. An\nagent aims to steal a gem (colored yellow),\nnavigating a maze and colored opening locks.\n(b): Heist achievement hierarchy. The agent\ncan only reach the gem after successively un-\nlocking all locks in order.\nHeist.\nHeist is part of the OpenAI Proc-\ngen (Cobbe et al., 2020) benchmark. In this en-\nvironment, agents must successfully ‘steal’ the\ngem after navigating a maze and opening all\nlocks (see Fig. 4). The gem is behind three lay-\ners of color-coded locks, each requiring that the\nprevious lock be unlocked first (e.g., to unlock\nthe green lock, the blue lock must first be un-\nlocked). Following Moon et al. (2023), the final\nscore is calculated as the average success of the\nagent in stealing the gem in 100 test episodes\nin 10 different seeds (i.e., 1,000 runs in total).\nFor agent training, we use a total of 5M steps\nin the LLM-generated environments (i.e., 5M\nHeistEnvGen steps) and a total of 20M in the ac-\ntual Heist environment. As the game only provides scores on the final objective (’steal gem‘)\nand the game is simple enough for the LLM-generated environments to cover all scenarios\nwith one generation, we only use NCycle = 1 training cycle.\n6\n Models\nDescription\n# LLM calls\n# Agent Params\nScore (%)\nReward\nHuman∗\n50.5 ± 6.8\n14.3 ± 2.3\nRandom∗\n1.6 ± 0.0\n2.1 ± 1.3\nELLM* (Du et al., 2023)\n5M step PT in Crafter w/ Codex reward\n5M\n62M\n-\n6.0 ± 0.4\nLSTM-SPCNN∗ (Stani´c et al., 2023)\n135M\n11.7 ± 0.8\n9.3 ± 0.2\nDreamerV3∗ (Hafner et al., 2023)\n201M\n14.8 ± 1.4\n10.9 ± 0.5\nMuZero + SPR∗ (Walker et al., 2023)\n150M step PT in Crafter w/ RND reward\n54M\n16.4 ± 1.5\n12.7 ± 0.4\nSPRING* (Wu et al., 2023)\n9 queries to call GPT-4 per step\n2.7K†\nUnknown\n27.3 ± 1.2\n12.3 ± 0.7\nPPO (Moon et al., 2023)\n4M\n15.5 ± 0.6\n10.5 ± 0.6\nPPO (Moon et al., 2023)\n0.96M step PT in Crafter\n4M\n26.4 ± 2.1\n12.1 ± 1.0\nAD* (Moon et al., 2023)\n9M\n21.8 ± 1.4\n12.6 ± 0.3\nAD (Moon et al., 2023)\n0.96M step PT in Crafter\n9M\n31.8 ± 0.7\n13.3 ± 1.2\nPPO + EnvGen (Ours)\n0.96M step PT w/ CrafterEnvGen\n4\n4M\n32.2 ± 0.6\n12.6 ± 0.6\nAD + EnvGen (Ours)\n0.96M step PT w/ CrafterEnvGen\n4\n9M\n35.3 ± 0.7\n13.7 ± 0.8\nTable 1: Comparison of different agents in the Crafter (Hafner, 2022) environment. Following previous\nworks, we report the geometric mean of success rates across its 22 achievements and rewards for 1M\nCrafter steps. We experiment with EnvGen on two models, PPO and Achievement Distillation. *:\nscores from the Crafter Scoreboard (Hafner, 2022) and Moon et al. (2023). †: average number of LLM\ncalls to run a single episode, according to SPRING (Wu et al., 2023). PT: Pretraining; AD: Achievement\nDistillation; ±: one standard deviation.\n3.2\nAgent Architectures\nOur base RL agent.\nFor both Crafter and Heist, we test the EnvGen framework with\na simple (CNN + linear layer) and lightweight (<5M) agent used in Moon et al. (2023),\nwhich is slightly modified from the agent architecture used in IMPALA (Espeholt et al.,\n2018). Following Moon et al. (2023), we train the agent with a PPO (Schulman et al., 2017)\nobjective. At every step, the agent takes an RGB image (surroundings for Crafter, entire\nmaze for Heist) as input and outputs the value estimates and policy (action probability). See\nFig. 3 (a) and Fig. 4 (a) for example visual inputs for agents. We provide additional model\nimplementation details in the appendix.\nBaseline methods.\nFor Crafter, we compare our method to two groups of recent baselines\n– (1) methods that use frequent (i.e., more than thousands of) LLM calls during training\nor inference: SPRING (Wu et al., 2023) (based on GPT-4 (OpenAI, 2023a)) and ELLM (Du\net al., 2023) (based on Codex (Chen et al., 2021)) and (2) methods that do not use an LLM\nduring training or inference: DreamerV3 (Hafner et al., 2023), MuZero + SPR (Walker et al.,\n2023), LSTM-SPCNN (Stani´c et al., 2023), PPO (Schulman et al., 2017), and Achievement\nDistillation (AD) (Moon et al., 2023). For Heist, we compare against the PPO agent. For the\nPPO and AD agents, we follow the implementation of Moon et al. (2023). See appendix for\nthe PPO/AD agent details.\n4\nResults and Analysis\nWe demonstrate the usefulness of the EnvGen method with comprehensive experiments\nand analysis. We first compare RL agents trained with EnvGen to different baseline methods\non Crafter, an open-world game with 22 hierarchical achievements (Sec. 4.1). Next, we\npresent a detailed analysis of the improvements that training with EnvGen environments\ncan give RL agents on long-horizon tasks (Sec. 4.2). Then, we analyze how the LLM-based\nenvironment adaptation can help an RL agent progressively improve the skills that the\nagent is weak at (Sec. 4.3). Moreover, we also compare an RL agent’s performance on Heist\n(a maze navigation game), with and without EnvGen environments (Sec. 4.4). Lastly, we\npresent various ablation studies on EnvGen design choices (Sec. 4.5).\n4.1\nComparison with State-of-the-art Methods on Crafter Environment\nSmall RL agent trained with EnvGen outperforms state-of-the-art baselines.\nOn the\nCrafter environment (described in Sec. 3.1), we compare a small PPO agent trained in\nCrafterEnvGen (i.e., Crafter environments generated with EnvGen) to state-of-the-art baseline\nmethods. As shown in Table 1, we find that a small (4M parameters) PPO agent with\n7\n Collect Coal\nCollect Diamond\nCollect Drink\nCollect Iron\nCollect Sapling\nCollect Stone\nCollect Wood\nDefeat Skeleton\nDefeat Zombie\nEat Cow\nEat Plant\nMake Iron Pickaxe\nMake Iron Sword\nMake Stone Pickaxe\nMake Stone Sword\nMake Wood Pickaxe\nMake Wood Sword\nPlace Furnace\nPlace Plant\nPlace Stone\nPlace Table\nWake Up\n0.001\n0.01\n0.1\n1\n10\n100\n1.96M Crafter Steps\n0.96M steps in Crafter EnvGen + 1M Crafter Steps (Ours)\nSuccess Rate (%)\nFigure 5: Success rates for all the Crafter achievements of two PPO agents (Moon et al., 2023) – (1)\nBaseline: trained in Crafter for 1.96M steps, and (2) Ours: trained in 0.96M steps in CrafterEnvGen and\n1M in Crafter.\nEnvGen achieves an average score of 32.2% and significantly outperforms the baselines\n(and also in terms of the average reward). Note that some baseline agents have many more\nparameters or pretraining steps such as SPRING (27.3%) that directly employs GPT-4 as\nagent, and MuZero + SPR (16.4%) that uses 150M pretraining steps. Also, note that our\nmethod only uses orders of magnitude fewer LLM calls (only 4) than works like SPRING\n(2.7K on average) and ELLM (5M). Due to the large number of LLM calls, SPRING costs\naround $270 USD to run an agent in a single episode, whereas EnvGen only costs a couple\nof cents total for any number of episodes (see appendix for cost details). Moreover, we find\nthat EnvGen can help train another small RL agent – Achievement Distillation (AD) (Moon\net al., 2023) – to achieve an even higher score (35.3%).\n4.2\nDetailed Achievement Analysis on Crafter Environment\nNext, we analyze where EnvGen improves the overall score by checking individual achieve-\nment success rates in detail. For this, we compare the same PPO agent architecture trained\nwith different setups: (1) an agent trained on Crafter for 1.96M steps and (2) an agent trained\non CrafterEnvGen for 0.96M steps (0.24M steps × 4 training cycles, see Sec. 2.2) and then\ntrained on Crafter for 1M steps. We measure the success rate (Fig. 5) of each achievement\nand unlocking speed (Fig. 6) of iron tools in the last 1M training steps, and discuss the\nresults below.\nEnvGen helps RL agents to tackle challenging long-horizon achievements.\nFig. 5 shows\nthat training in CrafterEnvGen improves scores of several achievements. Notably, training in\nCrafterEnvGen significantly improves the scores of long-horizon achievements (with many\nprerequisites; see Fig. 3) such as ‘make stone pickaxe’, ‘make iron pickaxe’, and ‘make iron sword’.\nFig. 6 shows that after unlocking the stone pickaxe, the RL agent trained in CrafterEnvGen is\nsignificantly faster in unlocking iron tools compared to the RL agent trained only in Crafter\n– 40K vs. 135K steps for ‘make iron pickaxe’ and 192K vs. 925K steps for ‘make iron sword’. In\nthe appendix, we additionally show the comparison between two AD (Moon et al., 2023)\nagents, where the agent trained in CrafterEnvGen can also improve the success rate of another\nchallenging long-horizon achievement – ‘collect diamond’.\n4.3\nAdaptation of Training Environments Helps the Agent Improve Weaker Skills\nFig. 7 shows how the LLM adaptively generates new training environments based on the\nintermediate performance of our PPO-based RL agent. In the intermediate performance\nplots, we compare the baseline agent trained only in Crafter and our RL agent trained in\nCrafterEnvGen. In the cycle 2, given the feedback that the current RL agent is not good at\n8\n Make Iron \nSword\nMake Iron \nPickaxe\nMake Stone \nPickaxe\nMake Iron \nPickaxe\nMake Stone \nPickaxe\nMake Iron \nSword\nFigure 6: Unlock times (the first moment when the agent completed an achievement) for three long-\nhorizon achievements (‘make stone pickaxe’, ‘make iron pickaxe’, and ‘make iron sword’) of two PPO\nagents (Moon et al., 2023) – (1) Baseline: trained in Crafter for 1.96M steps, and (2) Ours: trained for\n0.96M steps in CrafterEnvGen and for 1M steps in Crafter. The plot shows the last 1M training steps out\nof 1.96M steps. Our agent that was trained in CrafterEnvGen environments unlocks the achievements\nmuch quicker than the baseline agent that was only trained in the Crafter environment. As shown in\nFig. 3, these achievements have many prerequisites and require long-term planning.\nCollect\nCoal\nMake Stone \nPickaxe\nCycle 2\nHere are new environments that \ncan help the agent improve the \nlow scores:\n...\nEnvironment 1: Introduction to \nMining and Crafting\n```json\n{\n \"target_biome\": \"mountain\",        \n \"wood_pickaxe\": 1,\n ...\n}\n```\nPurpose: This environment is \ndesigned to teach the agent \nhow to mine coal ...\n...\nFeedback from \nprevious cycle\nCollect coal is 2%\nMake stone pickaxe is 1% \n…\nLLM\nCycle 3\nHere are new environments that \ncan help the agent improve the \nlow scores:\n...\nEnvironment 2: Introduce \ncombat and advanced tool \ncrafting\n```json\n{\n \"wood\": 3,\n \"stone\": 2,\n ...\n}\n```\nPurpose: With stone in the \ninventory, the agent is nudged \nto craft a stone pickaxe, a \ntask it hasn't learned yet ...\n...\nFeedback from \nprevious cycle\nCollect coal is 22%\nMake stone pickaxe is 1%\n…\nPerformance \nfrom Cycle 1\nPerformance \nfrom Cycle 3\nPerformance \nfrom Cycle 2\nTrained in \nCrafter\nTrained in \nCrafterEnvGen (Ours)\nLLM\nCollect\nCoal\nMake Stone \nPickaxe\nCollect\nCoal\nMake Stone \nPickaxe\nFigure 7: Adaptation of training environments based on agent performance over EnvGen cycles. At\nthe end of each cycle, the RL agent’s performance is given to the LLM as feedback (e.g., ‘Collect coal is\n2%’). The LLM uses the feedback to adaptively generate new environments that can help the agent\nprogressively tackle skills it was previously weak at. As the training proceeds, our RL agent trained\nwith EnvGen shows more rapid improvements than the baseline agent trained only in Crafter, by\nadaptively focusing the learning on previously weaker skills (i.e., ‘collect coal’ and ‘make stone pickaxe’).\ncollecting coal, the LLM could generate an environment to help the agent focus on the skill,\nimproving the agent performance in ‘collect coal’. Likewise, in the cycle 3, given the feedback\nthat the agent is weak at making stone pickaxe, the LLM could generate an environment to\nhelp the agent more easily craft the stone pickaxe, helping the agent improving the score in\n‘make stone pickaxe’. Powered by the adaptive LLM environment generation of EnvGen, our\nagent learns to unlock these two achievements significantly faster than the baseline agent.\n4.4\nEvaluation on Heist Environment\nEnvGen can generalize to Heist.\nWe also evaluate the effectiveness of EnvGen framework\nwith another game environment – Heist, a maze navigation game described in Sec. 3.1.\nWe compare the PPO-based agent trained with and without EnvGen (i.e., HeistEnvGen\n9\n Model\n# Training Steps in HeistEnvGen\n# Training Steps in Heist\nScore (%)\nReward\nPPO\n-\n25M\n25.9 ± 13.2\n4.1 ± 1.8\nPPO + EnvGen (Ours)\n5M\n20M\n37.7 ± 7.50\n5.5 ± 0.9\nTable 2: Evaluation results on Heist. Scores are computed as the average success rate over 100 test\nepisodes over 10 different seeds.\n# Training Steps in CrafterEnvGen\n# Training Steps in Crafter\nScore (%)\nReward\n(Total 1.24M steps)\n-\n1.24M\n21.1 ± 2.3\n11.0 ± 0.9\n0.12M\n1.12M\n22.3 ± 1.5\n11.6 ± 0.8\n(Total 1.48M steps)\n-\n1.48M\n21.9 ± 2.1\n11.4 ± 0.9\n0.24M\n1.24M\n27.9 ± 1.2\n12.4 ± 0.7\n(Total 1.96M steps)\n-\n1.96M\n26.4 ± 2.1\n12.1 ± 1.0\n0.48M\n1.48M\n32.2 ± 0.6\n12.6 ± 0.6\nTable 3: RL agents trained in CrafterEnvGen environments vs. agents trained only in the Crafter\nenvironment. We calculate the scores based on the last 1M training steps in Crafter.\nenvironments). Table 2 shows that training an agent with HeistEnvGen environments is\neffective in improving performance by increasing average scores (25.9% → 37.7%) and\nrewards (4.1% → 5.5%), while also stabilizing training by reducing the score variance (i.e.,\nstandard deviation goes down 13.2% → 7.5%).\n4.5\nAdditional Analysis and Ablation Studies\nIn the following, we show comprehensive ablation studies of EnvGen method: EnvGen\nvs. longer training in the original environment, dynamically updating LLM environments\n(i.e., using adaptive environments) vs. using a fixed LLM environment, different LLMs\nfor generating environments, frequency of environment updates, the number of LLM\nenvironments, and the ratio of training steps in the LLM environments to the original\nenvironment. Unless otherwise noted, we use the PPO-based agent (Moon et al., 2023)\n(described in Sec. 3.2) on the Crafter (Hafner, 2022) benchmark (described in Sec. 3.1) with\n0.96M steps in CrafterEnvGen and average results for 30 runs (10 different seeds, 3 different\ninitial environments).\nEnvGen vs. longer training in the original environment.\nTable 3 shows that when given\nan equivalent # of total training steps, the agents trained with CrafterEnvGen environments\noutperform the agents only trained with Crafter (e.g., 22.3% vs. 21.1% for 1.24M total steps).\nAlthough the agent performances tend to improve with longer training steps in both settings,\ntraining with EnvGen shows stronger performance gains than only training longer in Crafter\n(e.g., 32.2% vs. 26.4% for 1.96M total steps).\nCrafterEnvGen environments during training\nScore (%)\nReward\nFixed\n29.9 ± 0.9\n12.6 ± 0.8\nUpdated based on RL agent performance (default)\n32.2 ± 0.6\n12.6 ± 0.6\nTable 4: RL agents trained in CrafterEnvGen environments that are (1) fixed during training vs. (2)\nadaptively updated based on intermediate agent performance. Both agents are trained with 0.96M\nsteps in CrafterEnvGen and 1M steps in the Crafter environment. For (2), the LLM adaptively generates\nthe environments for NCycle = 4 times during training.\n10\n LLM\nScore (%)\nReward\nDeepseek Coder 33B Instruct\n26.3 ± 0.9\n12.1 ± 0.8\nGPT-3.5-Turbo\n21.5 ± 2.8\n11.6 ± 1.0\nGPT-4-Turbo (default)\n29.9 ± 0.9\n12.6 ± 0.8\nTable 5: Ablation of employing different LLMs to generate the environments. Agents are trained with\n0.96M steps in CrafterEnvGen and 1M steps in the Crafter environment.\nEnvironment Update Frequency\n# Training cycles NCycle\nScore (%)\nReward\nEvery 0.012M steps\n40 cycles\n30.8 ± 0.7\n12.8 ± 0.6\nEvery 0.06M steps\n8 cycles\n32.1 ± 0.5\n12.7 ± 0.8\nEvery 0.12M steps (default)\n4 cycles\n32.2 ± 0.6\n12.6 ± 0.6\nTable 6: Different frequencies to give feedback to the LLM and update the environments (see Sec. 2 for\ndetails). Agents are trained with 0.96M steps in CrafterEnvGen and 1M steps in Crafter environment.\nAdaptive environments: Fixed. vs. Updated based on RL agent performance.\nTable 4\nshows that using LLM environments that are adaptively updated based on intermediate\nagent performance to improve weaker skills results in overall higher scoring agents than\njust using the initial LLM environments for the whole training (32.2% vs. 29.9%). These\nresults indicate the effectiveness of the agent feedback and environment updating (step 4\ndescribed in Sec. 2).\nDifferent LLMs to generate environments.\nTo figure out which LLM can generate more\nuseful training environments, we experiment with three different LLMs (GPT-4-Turbo,\nGPT-3.5-Turbo (OpenAI, 2023b), and Deepseek Coder 33B Instruct (Guo et al., 2024)) and\nuse NCycle = 1 (i.e., fixed environment). Table 5 shows that environments generated by\nGPT-4-Turbo outperform that of other LLMs including GPT-3.5-Turbo and Deepseek Coder\n33B Instruct. We see that GPT-3.5-Turbo performs the worst with only a score of 21.5%,\nwhile Deepseek 33B Instruct is able to get several points higher (26.3%) and GPT-4-Turbo,\nour default LLM, gets a few extra points (29.9%).\nFrequency of LLM feedback / environment updates.\nTable 6 shows that updating the\nLLM environments at every 0.12M steps results in the best agent performance. While\nincreasing the cycles of environment feedback beyond 4 does not improve further, we\nfind that updating environments with feedback always helps improve the RL agent’s\nperformance compared to training only with the original Crafter environment (26.4%) or\nthe fixed LLM environment (29.9%).\nNumber of LLM environments.\nTable 7 shows that changing the number of environments\ngenerated by the LLM at each cycle (i.e., 1, 2, 4, and 8) can slightly affect agent performance.\nWhile training with four environments produces the highest result, training with environ-\nments generated with any of the tested configurations improves performance over training\nonly with the original Crafter environment (26.4%).\nRatio of training steps: LLM environments vs. original environment.\nAs mentioned\nin Sec. 2.2, in EnvGen, we train the RL agent in LLM environments (step 2) and then\nin the original environment (step 3) to mitigate the agent from overfitting to the LLM\nenvironments. We experiment with different ratios of training steps in LLM environments\n(i.e., CrafterEnvGen) compared to training steps in the original Crafter environment (e.g., 2:1\nindicates that for every two training steps in CrafterEnvGen, the RL agent gets one training\nstep in Crafter). As shown in Table 8, while different ratios do not result in big differences,\nthe default 1:1 ratio provides the highest scores.\n11\n # LLM environments\nScore (%)\nReward\n1\n30.8 ± 0.5\n12.8 ± 0.8\n2\n29.1 ± 0.6\n13.0 ± 0.6\n4 (default)\n32.2 ± 0.6\n12.6 ± 0.6\n8\n31.0 ± 0.8\n12.9 ± 0.8\nTable 7: Different number of LLM environments being generated by the LLM per cycle. Agents are\ntrained with 0.96M steps in CrafterEnvGen and 1M steps in the real Crafter environment.\nRatio of Training Steps in CrafterEnvGen : Crafter\nScore (%)\nReward\n5:1\n30.3 ± 0.6\n12.3 ± 0.9\n2:1\n30.1 ± 1.1\n12.8 ± 0.7\n1:1 (default)\n32.2 ± 0.6\n12.6 ± 0.6\nTable 8: Different ratios of training steps in LLM-generated environments (CrafterEnvGen) compared to\ntraining steps in the original Crafter environment (e.g., 2:1 indicates that for every two training steps\nin CrafterEnvGen, the RL agent gets one training step in Crafter). We keep the total number of training\nsteps constant at 1.96M.\n5\nRelated Works\nLLMs as open-world game agents.\nAs LLMs have shown rapid progress in various\ndomains (Brown et al., 2020; OpenAI, 2023a; Touvron et al., 2023a;b; Chowdhery et al.,\n2023; Anil et al., 2023; Gemini Team, 2023), recent works study using LLMs to create action\nplans (i.e., a list of subgoals or skills to target) for embodied agents in open-world games\nlike Minecraft and Crafter (Hafner, 2022). Most of these methods require calling LLMs\nfrequently (e.g., at every step) for planning the next steps (Yuan et al., 2023; Wang et al.,\n2023c; Wu et al., 2023; Wang et al., 2023a;d; Zhao et al., 2023). Other methods, such as Li et al.\n(2024); Kwon et al. (2023); Ma et al. (2023); Du et al. (2023), have used LLMs to create/adjust\nrewards to train agents. Although these works show initial promising results leveraging the\nworld knowledge of LLMs to tackle long-horizon tasks, iteratively calling LLMs throughout\nepisodes is prohibitively slow and expensive (e.g., running a single episode in the Crafter\nenvironment with SPRING (Wu et al., 2023) costs around $270 USD as they have 2.7K LLM\ncalls on average). EnvGen framework proposes an alternative: calling LLMs only a few\ntimes (e.g., 4) throughout the learning process to create training environments that focus on\nhelping the RL agent progressively improve the skills that the agent is weak at.\nReward designs in reinforcement learning.\nFinding good action trajectories is critical\nin reinforcement learning (RL) (Sutton & Barto, 2018). While classic random exploration\nalgorithms such as epsilon-greedy (Watkins, 1989) work well in simple settings such as\nmulti-armed bandit, it is not the case for hard exploration problems where the environment\ngives very sparse rewards (Weng, 2020). A line of work studies how to augment the\noriginal (extrinsic) rewards from the environment with intrinsic rewards that encourage\nexploration (Bellemare et al., 2016; Burda et al., 2018). While such intrinsic rewards can help\nRL agents discover novel states and improve their knowledge about the environment, it\noften requires long pretraining and does not guarantee that the intrinsic reward can help the\ntarget task. Another recent line of work studies using LLMs to adjust reward functions to\nhelp RL agents progressively learn certain tasks (Li et al., 2024; Kwon et al., 2023; Ma et al.,\n2023; Du et al., 2023). Instead of designing new rewards, in EnvGen, an LLM adaptively\ngenerates training environments that can help the RL agent learn multiple skills it is weak\nat with fewer training steps than in the original environment; reward design could be\ncomplementary to our method.\nDeep learning-based game/simulator content generation.\nProcedural content generation\n(PCG) for games is about the automatic generation of levels, landscapes, items, rules, quests,\nor other types of game contents (Shaker et al., 2016). While traditional PCG methods are\nbased on search/solver/rule/grammar-based methods, recent works study applying deep\nlearning methods such as GAN (Goodfellow et al., 2014) for PCG (Liu et al., 2021; Kumaran\n12\n et al., 2020; Schubert et al., 2022). Several works have recently explored using LLMs to gen-\nerate game content such as difficulty levels (Sudhakaran et al., 2023; Todd et al., 2023) and\nscenes/environments (Kumaran et al., 2023; Wang et al., 2023b; Afshar & Li, 2024). While\nthese works study how to help developers create extra/new game content in simulators,\ngenerated environments may not be good for teaching agents how to play/be better in\nthe original environment. Our work studies using LLMs to adaptively generate training\nenvironments that teach multiple useful skills to produce better-performing embodied RL\nagents. Beyond game content generation, several recent works investigate visually augment-\ning vision-and-language navigation (VLN) simulators (e.g., rendering the environments\nwith different styles) using image generation models (Li et al., 2022b; Wang et al., 2023e; Li\n& Bansal, 2023). Such works could complement our LLM-based environment generation\n(e.g., rendering our LLM-generated environments with diverse colors and textures).\n6\nConclusion\nWe propose EnvGen, a novel framework to improve embodied agent performance by utiliz-\ning the world knowledge of LLMs to adaptively generate customized training environments\nthat progressively teach agents different skills more effectively. In EnvGen, we give an LLM\na prompt describing a game/simulator and ask the LLM to generate the configurations to\ncreate new environments that can teach different skills. Next, we train the agents in the\nLLM-generated environments and give feedback to the LLM by testing the agent perfor-\nmance in the original environments, and then ask the LLM to update the environments to\nteach agents skills they are weaker at. We experiment with two different games, Crafter and\nHeist, and find that our EnvGen framework effectively can increase agent performance. We\nalso demonstrate that training in LLM-generated environments can be more effective than\njust training longer in the original environments when learning long-horizon tasks. More-\nover, we show that a lightweight model (< 5M parameters) trained with LLM-generated\nenvironments can even outperform an LLM agent, with significantly fewer LLM calls. We\nalso show comprehensive analyses of our results and ablation studies validating the design\nchoices of EnvGen framework. We hope our work can guide future works in leveraging\nLLMs for training embodied agents.\nAcknowledgments\nWe thank Elias Stengel-Eskin for the thoughtful discussion. This work was supported\nby DARPA ECOLE Program No.\nHR00112390060, NSF-AI Engage Institute DRL-\n2112635, DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031, ARO Award\nW911NF2110220, ONR Grant N00014-23-1-2356, and a Bloomberg Data Science Ph.D. Fel-\nlowship. The views contained in this article are those of the authors and not of the funding\nagency."
                },
                "method": "vector_search",
                "rerank_score": 0.9402434135995513
            },
            {
                "_index": "papers_search_index",
                "_id": "6afb81ea6b05e542730b1fbad9f533d0",
                "_score": 0.54389805,
                "_ignored": [
                    "references.keyword"
                ],
                "_source": {
                    "item_id": "6afb81ea6b05e542730b1fbad9f533d0",
                    "title": "Language Models can Solve Computer Tasks",
                    "authors": {
                        "authors": [
                            {
                                "name": "Geunwoo Kim",
                                "work": "University of California, Irvine",
                                "contact": "kgw@uci.edu"
                            },
                            {
                                "name": "Pierre Baldi",
                                "work": "University of California, Irvine",
                                "contact": "pfbaldi@ics.uci.edu"
                            },
                            {
                                "name": "Stephen McAleer",
                                "work": "Carnegie Mellon University",
                                "contact": "smcaleer@cs.cmu.edu"
                            }
                        ]
                    },
                    "date": "2023-11-16",
                    "abstract": "Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting’s effectiveness in enhancing LLMs’ reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.",
                    "body": "\n1\nIntroduction\nA long-standing goal in artificial intelligence has been to create generally-intelligent agents that can\naccomplish cognitive tasks as well as humans. Such agents should be able to solve any computer task\na human can by communicating via natural language. By automating repetitive tasks and providing\nassistance in complex problem-solving, generally-intelligent virtual agents may radically increase\nproductivity.\nRecently, large language models (LLMs) have shown remarkable in-context learning capabilities\nacross a variety of domains and tasks [12, 69, 5, 17, 26, 64, 8, 46, 6]. Although LLMs can impressively\nmanipulate text and can use high-level API tools [59, 48, 41], previous approaches to using LLMs\nthat directly take keyboard and mouse actions on computers have had difficulty compared to imitation\nlearning and reinforcement learning approaches [24]. LLMs that take keyboard and mouse actions on\ncomputers face a number of obstacles, such as ensuring that generated actions are task-appropriate\n(task grounding), feasible in the agent’s current state (state grounding), and admissible to be executed\n(agent grounding).\n∗Corresponding author.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2303.17491v3  [cs.CL]  16 Nov 2023\n Figure 1: MiniWoB++ environment. Every task contains a natural language prompt in yellow. The\nagent then uses keyboard strokes and mouse clicks to accomplish the task.\nThe previous best-performing approaches for taking actions on computers have not used LLMs.\nInstead, they have trained networks from scratch to predict actions given prompts and screenshots or\nDOM information, either via supervised learning (SL) from expert demonstrations, reinforcement\nlearning (RL) on a handcrafted reward signal, or both (SL+RL) [30]. Although SL+RL works well\non a number of individual computer tasks, since it requires expert data and a reward function for\nevery task, it has not been shown to generalize to novel tasks in a few-shot setting.\nIn this work, we show that a pre-trained LLM agent can successfully execute computer tasks\nguided by natural language. Our method employs a simple prompting scheme, which we call\nRecursive Criticism and Improvement (RCI), that significantly outperforms existing LLM methods\nfor automating computer tasks. RCI works by first having the LLM generate an output based on\nzero-shot prompting. Then, RCI prompts the LLM to identify problems with the given output. After\nthe LLM has identified problems with the output, RCI prompts the LLM to generate an updated\noutput.\nWhen applying RCI to computer tasks, we improve task grounding, state grounding, and agent\ngrounding sequentially. Firstly, task grounding prompts the LLM with the task text, instructing it to\ngenerate a high-level plan. Secondly, state grounding connects high-level concepts derived from the\ntask grounding step with actual HTML elements present in the current state, subsequently outputting\nthe appropriate action. Finally, agent grounding ensures the correct formatting of the action output\nobtained from the state grounding step. RCI is applied to each of these three steps; however, we find\nthat critiquing the state-grounding step is only necessary once.\nWe evaluate the RCI approach on the MiniWoB++ benchmark [61], and show it surpasses existing\nSL, RL, and LLM approaches. Furthermore, it proves itself to state-of-the-art compared to existing\nmethods, using only a small number of demonstrations per task instead of tens of thousands, and\nwithout relying on a task-specific reward function. This significant reduction in required demon-\nstrations and the elimination of task-specific reward functions make our method more practical and\naccessible for new tasks. Furthermore, as the capabilities of LLMs continue to improve, one can\nexpect the performance of our method to improve as well.\nIn addition to its success in automating computer tasks, we also showcase the effectiveness of RCI\nprompting in enhancing the reasoning abilities of LLMs on a suite of natural language reasoning tasks.\nWhen external feedback is given, our method achieves a significant performance increase over zero-\nshot prompting and slightly improves upon chain-of-thought [73] (CoT) prompting. Interestingly,\nRCI and CoT have a synergistic effect, and their combination outperforms all other methods.\nIn summary, our work presents a new powerful and practical approach to enabling LLM agents to\nexecute computer tasks guided by natural language. The RCI prompting scheme not only outperforms\n2\n A:\nValerie's brother earns $10,000 per month, while Valerie herself\nearns half of that amount. Together, they earn a total of $15,000 per\nmonth. Their mother's monthly earnings amount to $30,000, which\nis twice the combined salary of Valerie and her brother. Thus, the\ntotal amount of money they all have together is $30,000. (Wrong)\n<Few shot examples>\nA:\nValerie earns $5000 per month, half of her brother's earnings,\nmaking his monthly income $10000. Combined, Valerie and her\nbrother earn $15000 per month. Their mother earns double their\ncombined salary, amounting to $30000 per month. In total, they\nhave $30000 collectively. (Wrong)\nA: Let's think step by step. First, Valerie's brother earns 2x and\nValerie earns x (half of her brother's earnings). Next, their mother's\nincome is twice the combined earnings of Valerie and her brother. In\nthis case, their total income amounts to $30,000. (Wrong)\nA: Valerie's brother earns $10,000 per month, while Valerie herself\nearns half of that amount. Together, they earn a total of $15,000 per\nmonth. Their mother's monthly earnings amount to $30,000, which\nis twice the combined salary of Valerie and her brother. Thus, the\ntotal amount of money they all have together is $30,000.\nReview your previous answer and find problems with your answer.\nAll individuals, including Valerie, her brother, and mother, should be\ntaken into account when calculating the overall sum of money.\nBased on the problems you found, improve your answer.\nValerie's brother earns $10,000 per month, while Valerie herself\nearns $5,000. Together, they earn a total of $15,000 per month. Their\nmother earns double their combined salary, which amounts to\n$30,000 per month. In total, they have $45,000 collectively. (Correct)\nFew-Shot-CoT\nZero-Shot-CoT\nRCI\nZero-Shot\nQ: Valerie earns $5000 per month, 1/2 of what her brother earns. If their mother earns twice their combined salary, what's the total amount of\nmoney they all have together?\n<Initial output generation>\n<Critique>\n<Improve>\nFigure 2: Illustrative examples of explicit RCI prompting and baseline prompting approaches on\nthe GSM8K dataset. RCI prompting effectively addresses logical errors that arise in the baseline\nprompting approaches. Prompts text is displayed in violet color.\nprevious methods in computer tasks, but also improves reasoning abilities for LLMs more broadly,\nmaking it a significant contribution in the development of intelligent agents.\n2\nMethods\n2.1\nRCI Prompting\nThe self-critiquing ability of LLMs has demonstrated that LLMs can find errors in their own output\nby themselves [58, 20, 3]. In light of this, we introduce a simple reasoning architecture called RCI\nprompting, where we prompt LLMs to find problems in their output and improve the output based\non what they find. This architecture is designed to further enhance the reasoning ability of LLMs\nby inserting a critique step before generating the final answer. Figure 2 compares example traces of\nRCI prompting and baseline prompting methods on GSM8K dataset where language models should\nanswer grade school math problems. While baselines elicit answers with a single step of prompting,\nRCI consists of two steps: criticize the previous answer (e.g., \"Review your previous answer and\nfind problems with your answer\") and improve the answer based on the critique (e.g., \"Based on\nthe problems you found, improve your answer\"). In this way, RCI prompting finds errors (e.g., the\noverall sum of money only considered Valerie and her brother) in the previous answer and generates\nan improved answer (e.g., money from Valerie’s mother is included in the total) conditioned on\nthe critique. The iterative process of RCI can be continued until specific conditions are satisfied,\nwhich could include receiving feedback from the environment, reaching the maximum predetermined\nnumber of iterations, or adhering to certain heuristics. We define two approaches for achieving RCI:\nexplicit RCI and implicit RCI. Explicit RCI includes the critique in the prompt to generate improved\noutput and implicit RCI updates the previous output directly without sampling a critique explicitly.\nExamples of explicit RCI and implicit RCI applied to computer tasks are illustrated in Figure 3 where\nthe action plan is improved based on an explicit critique while actions are updated without an explicit\ncritique. This process is described in more detail in the following section.\n2.2\nRCI for Computer Tasks\nIn this section we describe the application of RCI to computer tasks via a decomposition of action\nselection into three reasoning steps: task grounding, state grounding, and agent grounding. The first\nstep, task grounding, involves generating a plan for task-solving and conditioning actions on this plan,\nwith RCI being used to improve the plan’s success rate. The state grounding subsection discusses the\nimportance of grounding actions in the environment for language-based agents and how implicit RCI\nis used to refine task-grounded actions to be feasible in the current state. Lastly, the agent grounding\n3\n Explicit RCI\nHere is a plan to solve the task using\nthe instructions specified:\n1. Type \"rm [filename]\" command to\ndelete the file ending with \".rb\".\n2. Press \"enter\" key\nFind problems with this plan.\nThis plan does not specify the filename\nof the file ending with the extension\n.rb. Without knowing the filename, the\nagent cannot delete the file\nBased on this, the improved plan for the agent to\ncomplete the task are as follows.\n1. Type \"ls\" command to list all files in the terminal.\n2. Type \"rm [filename]\" command to delete the file\nending with \".rb\".\n3. Press \"enter\" key after typing each command to\nexecute the command.\nPlan\nCritique\nImprove\nAccording to the current\nplan,\nthe\nnext\nproper\ninstruction should be `type\n\"rm [filename]\"`.\nConsidering the output on the webpage,\nthe specific instruction for solving the\ntask should be `type` instruction with\n\"rm code.rb\" command.\nThe task is to use the terminal below to delete a file ending with the extension .rb.\nTask\nTherefore, the single instruction\nthat matches one of the regular\nexpressions is `type \"rm code.rb\"`\n3rd step\n2nd step\n1st step\nInitial state\nCurrent state\nTask grounding\nState grounding\nAgent grounding\nImplicit RCI\nFigure 3: An illustrative execution trace of the agent for terminal tasks with RCI prompting. The\nlanguage model generates a step-by-step plan for the high-level task described in natural language,\nwhich in this case involves using the terminal to delete a file ending with \".rb\". We then run an\nexplicit RCI on this plan, where we sample an improved plan based on the critique and the previous\nplan, resulting in an improvement in the task-grounding of the plan. For each step, we first sample\nthe task-grounded action that follows the improved plan, and then the implicit RCI updates the\ntask-grounded actions sequentially to provide state-grounding and agent-grounding. Finally, the\nagent-grounded action is executed by the instruction-following agent on the environment. The\nprompts are highlighted, and the remaining text shows the outputs generated by the language model.\nstep focuses on ensuring that actions are admissible for the computer agent by employing implicit\nRCI and conditioning agent-grounded actions on the current state, task, and other grounded actions,\nwith a loop count set to optimize performance.\n2.2.1\nProblem Setting\nWe assume that we are given an instruction-following computer agent that can execute a set of\nadmissible actions given some natural language instructions. An instruction that is not part of the\nadmissible actions will be ignored. At every step, we receive a high-level natural language task\nprompt and a state of the environment. Given the current state and task, we sample the most probable\naction from LLMs. The generated natural language action is then fed into the computer agent.\nSampling the actions in a fully generative manner presents a challenge, as the actions must consider\nthe given task, feasibility in the current state, and admissibility for the computer agent simultaneously.\nTherefore, we propose decomposing this action sampling into three reasoning steps each of which\nconsiders task grounding, state grounding, and agent grounding. Task grounding improves actions to\nbe more effective in solving the given task, state grounding ensures the feasibility of actions in the\ncurrent state, and agent grounding considers the executability of actions given the specification of the\ncomputer agent. We first sample a step-by-step plan to solve the given task which improves the task\ngrounding. Next, the task-grounded action is sampled conditioned on the current state, task, and the\ngenerated plan. The state-grounded actions is generated conditioned on the task-grounded action.\nIf the task-grounded action is not executable by the computer agent, the agent-grounded action is\nsampled. For each sampling of grounded action, we use RCI prompting to make LLM consider some\nspecific information for grounding.\n2.2.2\nGrounding Language Model in Computer Tasks\nTask grounding.\nIn the action sampling process, the first step involves generating a plan of\nactionable steps for task solving from LLMs. Subsequently, actions are sampled from the same\nLLMs, taking into account the present state, task, and generated plan. The benefits of conditioning on\n4\n the plan for improved grounding of actions are twofold. First, it enables LLMs to identify the stage\nof task solving at which the agent is located, agent as a memory module. Second, we can perform\nexplicit RCI on the generated plan to further improve the plan’s success rate. Although the number of\nexplicit RCI loops can be arbitrary, we observe that a single pass of explicit RCI suffices for most of\nMiniWoB++ tasks.\nState grounding.\nIn language-based agents, grounding actions in the environment is a crucial step\nto enable real-world task performance. The aim of this phase is to enhance the task-grounded actions\nto be feasible in the current state. Although the actions generated in the preceding phase may align\nwith the task, they may lack the specificity required to be executed in the current context. For example,\nif the assigned task is to forward an email from Bob to Alice and the action obtained from the task\ngrounding phase is to click on an email from Bob in the email inbox, it is necessary to establish a\nconnection between the abstract concept of \"email from Bob\" and the concrete element, such as the\nemail heading, in the current webpage state represented by HTML. To achieve this goal, we perform\nthe implicit RCI and prompt the LLMs to consider the current state, which subsequently outputs\nrefined state-grounded actions. Moreover, the state-grounded action is additionally conditioned on\nthe task-grounded action. We avoid repeating the implicit RCI cycle more than once as it does not\nimpact the success rate based on our observations.\nAgent grounding.\nTo ensure the successful integration of language-based methodologies in\ndecision-making processes, it is imperative to establish a scalable framework that guarantees the\nadmissibility of actions derived from the language model. While the preceding steps of sampling\nproduce a state-grounded action that is both feasible and grounded in the task, it may not be executable\nby the agent due to issues such as improper formatting. To address this, Implicit RCI is employed,\nwhereby an agent-grounded action is sampled conditioned on the current state, task, task-grounded\naction, and state-grounded action. The LLMs are prompted to consider specifications of the computer\nagent. The implicit RCI is repeatedly run until the resulting action is executable, with a maximum\nloop count set to limit the number of iterations. Empirical analysis on MiniWoB++ tasks suggests\nthat setting the loop count to 3 yields optimal performance.\n3\nEvaluation\n3.1\nReasoning tasks\nIn our grounding enhancement process, RCI prompts the LLM to criticize its prior output, considering\nthe given context (e.g., current task, state, and agent), which ultimately leads to improved output. We\nfirst demonstrate the effectiveness of RCI prompts in augmenting the reasoning capabilities of LLMs\nacross a range of reasoning benchmarks. We compare RCI to Chain-of-Thought (CoT) prompting, a\nstate-of-the-art method recognized for its effectiveness in reasoning tasks.\nSpecifically, we compare our approach with Few-Shot-CoT [73] where a few chain-of-thought\ndemonstrations are given as examples in prompting, and Zero-Shot-CoT [33] that elicit multiple\nreasoning steps by simply adding \"Let’s think step by step\" to the prompt. Following Kojima et\nal. [33], our evaluation is conducted with 8 datasets from two categories of reasoning: arithmetic and\ncommonsense. Please refer to Appendix C.2 for a comprehensive depiction of the datasets. We use\nthe same experimental setting with their answer extraction method except that we use InstructGPT-3 +\nRLHF (gpt-3.5-turbo) as the underlying language model. We use the same prompts that CoT uses and\nwe also use the answer cleansing approach used in CoT, but we only used answer extraction prompting\nin zero-shot CoT experiments. We also use the same few-shot examples that were introduced in\n[73] to evaluate Few-Shot CoT’s performance on five arithmetic reasoning tasks. A threshold is\nestablished by setting the maximum number of RCI loops to two, terminating the loop once the\noutput aligns with the ground-truth data. We observed that in the absence of this external feedback\nmechanism, the RCI process is prone to false negative critics, subsequently leading to a decrease in\nperformance. Experimental results indicate that RCI without external feedback achieves zero-shot\nperformance in half of the benchmark tests, but underperforms in others, as shown in Appendix 17.\nComparison with Zero-Shot.\nRCI prompting is better at solving reasoning tasks compared to zero-\nshot prompting. Table 1 summarizes the accuracy of our approach (Zero-Shot + RCI) and standard\nzero-shot prompting for each reasoning benchmark. Zero-Shot + RCI substantially outperforms the\n5\n standard prompting in all benchmarks including arithmetic (GSM8K, MultiArith, AddSub, AQUA,\nSVAMP, SingleEq) and common sense (CommonSenseQA, StrategyQA) tasks. RCI prompting\neven achieves score gains from two arithmetic reasoning tasks (SingleEq and AddSub), which\ndo not require multi-step reasoning. This distinguishes our RCI prompting from the previous\nCoT prompting methods [73, 33] that are not useful in simple reasoning tasks. It is also worth\nnoting that RCI prompting achieves a significant performance gain in commonsense reasoning tasks\n(CommonSenseQA and StrategyQA). While Wei et al. [73] reported that only a substantially large\nPaLM (540B) model can benefit from Few-Shot-CoT, RCI prompting can provide performance gain\neven with a smaller InstructGPT-3 + RLHF (175B) model.\nArithmetic\nCommon Sense\nGSM8K\nMultiArith\nAddSub\nSVAMP\nSingleEq\nAQuA\nCommonSenseQA\nStrategyQA\nZero-Shot\n77.95\n94.48\n88.58\n80.70\n86.61\n60.23\n64.56\n48.81\nZero-Shot + RCI\n85.43\n97.64\n89.76\n84.65\n94.49\n67.32\n68.11\n61.81\nTable 1: RCI prompting increases the reasoning capability of LLMs on all of eight reasoning\nbenchmarks.\nComparison with Chain-of-Thought.\nThe performance results of RCI and CoT baselines on\narithmetic reasoning tasks are summarized in Table 2. Notably, Zero-Shot + RCI outperforms\nZero-Shot CoT and Few-Shot CoT without any CoT prompting in four tasks except MultiArith.\nIn MultiArith tasks, where most of the standard prompting’s answers are correct (96.06%), RCI\nprompting does not yield significant performance gains. RCI prompting has a synergistic collaborative\nimpact on the two CoT baselines. Namely, Zero-Shot CoT + RCI and Few-Shot CoT + RCI attain the\nhighest scores on four out of the five tasks. These findings suggest a promising avenue for future\nresearch: combining RCI with other prompting methods for CoT, such as self-consistency [58].\nGSM8K\nMultiArith\nAddSub\nSVAMP\nSingleEq\nZero-Shot\n78.35\n96.06\n85.83\n78.35\n91.34\nZero-Shot + RCI\n85.43\n97.64\n89.76\n84.65\n94.49\nZero-Shot CoT\n82.28\n96.85\n83.86\n79.92\n89.37\nZero-Shot CoT + RCI\n86.22\n97.24\n89.88\n85.83\n90.94\nFew-Shot CoT\n80.31\n98.82\n89.37\n83.46\n91.73\nFew-Shot CoT + RCI\n84.25\n99.21\n90.55\n87.40\n93.70\nTable 2: Chain-of-Thought prompting exhibits a synergistic effect when coupled with RCI prompting\nin arithmetic reasoning tasks.\n3.2\nComputer tasks\n3.2.1\nSetup\nMiniWoB++ benchmark suite.\nThe miniwob++ task suite is selected as the main benchmark to\nevaluate our computer agent. MiniWoB++ [36], an extension of MiniWoB [61], is a web-based\nsimulation environment that offers a diverse range of computer tasks, from simple button-clicking to\ncomplex compositional tasks requiring advanced reasoning, such as solving math problems. Its shared\naction space, including keyboard and mouse, and a common state space centered around HTML code\nenables our proposed agent to be thoroughly evaluated in ample tasks. Additionally, the varying levels\nof complexity between tasks enable a systematic evaluation of our work. The action space consists of\ntwo operations each of which controls the keyboard and mouse. The first action enables typing of\narbitrary characters or special keys such as Backspace and Enter. The second action involves moving\nand clicking the mouse, allowing the agent to interact with visible HTML elements on a webpage.\nAll actions can be executed through natural language instructions defined by regular expressions that\nare presented within the initial prompts provided to the LLMs. The regular expressions employed in\nour evaluation are presented in Appendix D. Our action space definition is similar to previous works,\n6\n CC-Net\nWebN-T5-3B\n(no history)\nWebN-T5-3B\n CC-Net\nOthers\nCC-Net\n(no dictionary actions)\n  CC-Net\nOurs\nSL SotA\nRL SotA\nSL + RL SotA\nModel\n0.0\n0.2\n0.4\n0.6\n0.8\nAverage Success Rate\n0.32\n0.45\n0.50\n0.57\n0.76\n0.85\n0.94\n0.94\n0.57\n0.67\n0.94\nMiniWoB++ Average Performance\nFew-Shot In-Context Learning (Ours)\nRL + SL\nSL\nRL\n(a)\n10\n1\n10\n2\n10\n3\n10\n4\nAmount of Demos per Task\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Success Rate\nCC-Net (SL)\nWebN-T5-3B\nWebN-T5-3B\n(no action history)\nCC-Net (SL + RL)\nWorld of bits\n(SL + RL)\nWorkflow guided exploration\n(SL + RL)\nOurs\nMiniWoB++ Average Performance\n vs. Amount of Data\n(b)\nFigure 4: (a) Average performance comparison with baselines. Our agent with RCI prompting\nachieves state-of-the-art performance in MiniWoB++ environment. The tasks that were included in\nthe averaging process are indicated in Table 18. (b) Relationship between performance and amount\nof expert training data. Our agent displays comparable performance to the current state-of-the-art\nscores on the MiniWoB++ benchmark, despite using the least amount of data.\nsuch as [25, 32, 36], in which clicking actions directly interact with HTML elements. However, for\ntyping actions, we extend beyond simple form-filling by using keyboard-based typing actions. Instead\nof relying on dictionary-based typing actions [30], where the agent simply chooses from a predefined\ndictionary of texts, our approach requires the agent to predict the proper text input. Our approach,\ntherefore, has a better generalization capability for diverse computer tasks. The state space of our\nagent consists solely of HTML code.\nModel choices.\nFor the purpose of evaluating the effectiveness of RCI prompting, multiple lan-\nguage models are used in our experiments. Specifically, we employ three models, namely, GPT-3\n(davinci) [5], InstructGPT-3 (text-davinci-002) [47, 72, 57], and InstructGPT-3 + RLHF (gpt-3.5-\nturbo, gpt-4) [47]. Unless otherwise specified, we primarily evaluate our computer agent with the\nInstructGPT-3 + RLHF models (gpt-3.5-turbo, gpt-4). Additionally, we use GPT-3 and InstructGPT-3\nmodels for ablation studies. All the models were obtained through the OpenAI API, and further\ndetails can be found in Appendix C.1.\nEvaluated tasks.\nWe employ a set of 55 tasks to enable fair comparisons with baselines, as previous\nworks are only evaluated on a subset of tasks consistently. Furthermore, to assess the performance of\nmodels on challenging tasks, we have selected tasks that involve free-form language typing actions,\nwhich have been reported to have an almost-zero success rate in previous works (e.g., terminal).\nNotably, certain commonly evaluated tasks in prior works are excluded due to the excessive length of\nHTML code for some UI components, which are described in Appendix C.3.\nMetrics\nConsistent with prior studies, our main evaluation criterion is the success rate, which\nmeasures the ability of our agent to actually complete the assigned task. This rate is calculated as the\nproportion of successful episodes, which are defined as those in which the agent receives a positive\nreward. We identified two modes of failure: the production of unexecutable actions and task failure.\nWhen the agent generates an unexecutable action following the implicit RCI step, it fails immediately.\nMoreover, an episode is considered unsuccessful when the agent, despite effectively executing the\nplan generated, is unable to accomplish the task and thus receives no reward.\n3.2.2\nOutperforming baselines on MiniWoB++ task suite\nWe present Figure 4a which summarizes the average success rate of our agent and baseline models\nover the MiniWoB++ benchmark. The results demonstrate significant outperformance of our approach\nover supervised learning models. Specifically, we observe a 41% higher score than the WebN-T5-\n3B, which employs a finetuned large language model with 12K expert demonstration data. Our\napproach also outperforms reinforcement learning approaches that require an order of magnitude\n7\n more interactions with the environment. Among all the baselines, our approach achieves the second\nhighest score. The sole model that surpasses our agent is the CC-Net, which involves co-training\nof reinforcement learning and imitation learning. However, a direct comparison with CC-Net is\nnot possible since it uses dictionary-based typing actions. In other words, CC-Net selects text from\na predefined list for typing actions in some tasks, while our approach is fully generative. Thus,\nCC-Net (without dictionary-based action) in Figure 4a serves as our appropriate comparison and we\noutperform it by 6%. The performance data for CC-Net (with no dictionary-based action) is obtained\nfrom the ablation study section in their paper [30].\nAnother comparative analysis is performed to evaluate the performance of our agent in contrast to the\nstate-of-the-art agents in three categories, namely supervised learning, reinforcement learning, and\na combination of both. To facilitate a fair comparison, we specifically isolate LLM-based state-of-\nthe-art approaches, which share similarities with our approach to solving computer tasks. The best\nper-task performance achieved by each category is then aggregated, and the outcomes are presented\nas SotA in Figure 4a. The result shows that our agent surpasses SotA by 37 percentage points in\nsupervised learning and by 27 percentage points in reinforcement learning. Notably, our proposed\nRCI prompting method outperforms the SotA LLM approach [24], even when the latter employs\nboth finetuning and few-shot examples in prompts. This outcome highlights the effectiveness of our\napproach in extracting vital knowledge for computer tasks from language models. Our agent even\nachieves a slight edge over SotA (less than 1 percentage point) in the combined use of supervised and\nreinforcement learning, which employs significantly more expert data and online interactions. We\nalso provide task-level performance comparisons in Figure 10, where tasks are arranged in ascending\norder based on the difference between our agent’s performance and the baseline. We observed three\nmain failure modes of our agent: (i) underperformance in tasks that require long-horizon planning\n(e.g., guess-number, search-engine, use-spinner), (ii) difficulty in selecting appropriate actions for\ntasks that require multi-step reasoning (e.g., tic-tac-toe, use-autocomplete), and (iii) lower scores in\ntasks that rely on visual rendering of HTML code to solve the task (e.g., count-shape). These failures\nare explained in more detail in Appendix F.\n3.2.3\nLowest sample complexity\nFigure 4b provides a comparative analysis of the total number of samples used in several models and\ntheir mean performance. We begin by discussing CC-Net [30] model, which employs 2.4 million\nexpert demonstrations (equivalent to 6,300 hours) collected from 77 human participants across 104\ntasks for behavior cloning. This amounts to an average of 23,076 demonstrations per task. In contrast,\nthe WebN-T5-3B [24] model uses 12,000 expert demonstrations to fine-tune its pre-trained T5 model.\nRather than directly updating model parameters with demonstration data, our approach involves\nintegrating two to three demonstrations into the prompt for in-context learning, which biases the\nmodel output without any parameter updates. This approach allows our agent to generalize to unseen\ntasks with only a handful of demonstrations. Our results show that our agent achieved a higher\nsuccess rate than all baselines, requiring 120x fewer samples than WebN-T5-3B and 11,000x fewer\nsamples than CC-Net. Given the challenges of obtaining expert demonstrations for computer tasks,\nour findings demonstrate the practicality of our approach in automating such tasks.\n3.2.4\nAblating the groundings\nThis section examines the impact of grounding improvement on task success rates. We conduct\nablations to isolate the contributions of task, state, and agent grounding improvements by eliminating\nRCI prompting at each stage. We categorize tasks by three different difficulty levels to provide a\nmore detailed understanding of the effects of grounding improvements across a diverse range of tasks.\nWe conducted a task grounding ablation by eliminating the plan sampling stage. This modification\nentails generating actions directly from the state, without the need for conditioning on a step-by-step\nplan. State grounding is evaluated by directly applying the agent-grounding update to task-grounded\nactions. Lastly, we ablate the implicit RCI of the agent grounding by letting the state-grounded\naction be the final output of the agent. Figure 5 illustrates the performance degradation resulting\nfrom each ablation of grounding. Our results indicate that each grounding contribution is essential\nto solving computer tasks, with each contributing almost equally to the overall success rate. The\nreason for this is partially due to the fact that the three methods of improving grounding are not\nmutually exclusive, but rather complementary, with one enhancement in grounding contributing to\nmultiple action groundings. Examples of cross-grounding improvement are provided in Appendix E.\n8\n Moreover, it has been observed that state grounding plays a crucial role in enabling an agent to use\nrelevant information during episodes, particularly in scenarios where the initial state does not offer\nsufficient information to accomplish the task, such as terminal task. Interestingly, task grounding\nsignificantly improves the success rate when a task requires a long-horizon action plan, such as\nthe click checkboxes large task. We also observe that agent grounding significantly enhances the\nfeasibility of actions. Notably, in simpler tasks, the success rate decreases by 60% in contrast to the\nbaseline without the agent grounding. This finding is of particular significance as it distinguishes\nour work from prior investigations [1, 28], which employ additional trained model components. In\ncontrast, our study solely relies on the reasoning ability of language models.\nBaseline\nWithout\ntask\ngrounding\nWithout\nstate\ngrounding\nWithout\nagent\ngrounding\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nEasy\nBaseline\nWithout\ntask\ngrounding\nWithout\nstate\ngrounding\nWithout\nagent\ngrounding\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nMedium\nBaseline\nWithout\ntask\ngrounding\nWithout\nstate\ngrounding\nWithout\nagent\ngrounding\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nHard\nFigure 5: Ablation analysis on the different types of grounding across tasks with varying degrees of\ndifficulty. The experimental design employs the use of InstructGPT-3 + RLHF model (gpt-3.5-turbo).\n3.2.5\nAblating the language model\nThe performance of our agent is contingent on the quality of the underlying pre-trained language\nmodels used, so enhancing language models can lead to an improvement in the agent’s performance.\nIn this section, we present a comparison of the agent’s performance using three distinct language\nmodels: GPT-3, InstructGPT-3, and InstructGPT-3 + RLHF (gpt-3.5-turbo). Our objective is to\ninvestigate the relationship between LLMs’ capability and their ability to solve MiniWoB++ tasks.\nThe experimental setting employed in Section 3.2.4 is replicated in this study. Figure 6 depicts\nthe average success rate of three language models on tasks of varying difficulty levels. Our results\nreveal that LLMs struggle to effectively complete tasks without instruction fine-tuning. This may\nbe attributed to the absence of intricate prompt engineering, as our observations have indicated\nthat GPT-3 displays sufficient competence in comprehending HTML code, regular expressions, and\nengaging in reasoning.\nInstructGPT-3\n+RLHF\n(Baseline)\nInstructGPT-3\nGPT-3\n0.00\n0.25\n0.50\n0.75\n1.00\nSuccess rate\nEasy\nInstructGPT-3\n+RLHF\n(Baseline)\nInstructGPT-3\nGPT-3\n0.00\n0.25\n0.50\n0.75\n1.00\nSuccess rate\nMedium\nInstructGPT-3\n+RLHF\n(Baseline)\nInstructGPT-3\nGPT-3\n0.00\n0.25\n0.50\n0.75\n1.00\nSuccess rate\nHard\nFigure 6: Ablation study on different language models across tasks of varying degrees of difficulty.\n4\nLimitations\nIn the course of our work, several limitations became apparent that may serve as potential avenues\nfor further research. One central concern is our primary focus on the InstructGPT-3 + RLHF models\n(gpt-3.5-turbo, gpt-4), leaving the generalization ability of RCI to other models unexplored. The\nversatility of RCI across diverse models remains a pertinent question, suggesting that future studies\nshould expand their scope to determine the robustness and adaptability of RCI. Handling lengthy\nHTML presents another challenge. The current model grapples with extensive HTML states. While it\nhas been suggested that efficiency might be bolstered by pruning HTML states to exclude non-critical\n9\n elements, the task itself is non-trivial. A fundamental constraint of LLMs is the limited context\nlength, which can hamper handling extensive HTML states effectively. Addressing this may require\narchitectural adjustments or novel parsing methods. Our agent’s action space, mainly restricted to\nclicks and typing, limits its web navigation capabilities. There’s a need to diversify its actions for a\nmore seamless experience. Furthermore, The agent’s focus on short-term decisions overlooks the\nnecessity for long-term strategy, especially in tasks requiring coordinated sequences. Broadening\nthis focus is essential for versatile applications. Lastly, the intricate UI components populating\ncontemporary websites present a challenge for LLMs to fully understand the HTML states. The\nsubtle nuances of such components, which may not be discernible through HTML alone, underscore\nthe need for adding more modalities to the state definition. Addressing these issues is crucial to\nenhance the RCI agent, making it more adaptable and efficient in practical applications.\n5\nDiscussion\nThis work is part of a growing literature showing that LLMs might be all you need for hard decision-\nmaking problems [76]. In contrast to imitation learning and reinforcement learning approaches,\nLLMs can solve novel tasks in a zero-shot or few-shot manner, and don’t require task-dependent\nexpert data or a reward function. Furthermore, we expect that as the capabilities of LLMs and\nfoundation models increase, our method will naturally improve as well. However, we find that\ncurrent capabilities of LLMs aren’t as powerful as task-dependent SL+RL approaches on some\ncomputer tasks. Also, RCI is more expensive to run compared to approaches that just sample once\nfrom the LLM. There are many avenues for future research in increasing the capacity of LLMs in\ndecision-making tasks. First, our experiments use LLMs on HTML code, but ideally methods based\non multimodal foundation models [16, 55, 2, 46] will be able to take actions based on text, images,\naudio, and video as input [4, 18, 44, 71]. Second, the results presented in this paper all use pre-trained\nLLMs. We expect the performance of our method to increase when using LLMs fine-tuned to solve\ncomputer tasks.\nImportantly, current LLMs are poor at reasoning tasks, such as playing tic-tac-toe, because they do\nnot think ahead. Although RCI improves reasoning capabilities in LLMs, there exists much work\nto be done on increasing the reasoning capabilities in LLMs. This will be crucial to accomplish\nhard cognitive tasks on computers that require thinking ahead. Similar to other prompting-based\napproaches for reasoning in LLMs, RCI can be viewed as using the LLM’s output to write to an\nexternal memory, which is later retrieved to choose an action. LLMs with memory have been\ndemonstrated to be computationally universal [60], meaning that in principle all that is needed to\nrun arbitrary programs is the right prompt. Since RCI represents a basic version of this powerful\nframework, we anticipate the development of more advanced RCI variations in the future. There\nis a vast array of potential methods that repeatedly feed the output of particular prompts into the\nLLM. For example, multiple different LLMs can simulate the information exchange between team\nmembers in an organization. This would enable the merging of diverse perspectives to tackle complex\nproblems. In such a context, incorporating game theory and multi-agent systems research could\nsignificantly enhance the overall performance. Reinforcement learning could be used to discover\neffective structures involving loops and prompts [81], either through human feedback or a given\nreward function. This optimization process can be further refined by exploring the space of potential\nloop and prompt structures, identifying those that yield the best results, and fine-tuning the model\naccordingly [75].\nAcknowledgement\nThis material is based upon work supported by the National Science Foundation under Grant #2127309\nto the Computing Research Association for the CIFellows 2021 Project."
                },
                "method": "vector_search",
                "rerank_score": 0.8253083097313677
            }
    ]
    :return:
    """
    prompt = ''
    if rerank_results:
        context = []
        for doc in rerank_results:
            if "authors" in doc["_source"]["authors"]:
                authors = [authors['name'] for authors in doc["_source"]["authors"]["authors"]]
            else:
                authors = [authors['name'] for authors in doc["_source"]["authors"]]
            doc_ = {
                "title": doc["_source"]["title"],
                "authors": authors,
                "date": doc["_source"]["date"],
                "abstract": doc["_source"]["abstract"],
                "body": doc["_source"]["body"]
            }
            context.append(doc_)
            pretty_context = json.dumps(context, indent=4, ensure_ascii=False)
            prompt = RAG_PROMPT.format(CONTEXT=pretty_context, QUESTION=question)
    else:
        prompt = question

    return prompt


if __name__ == '__main__':
    question = '大模型智能体能做什么呢？'
    rerank_results = search_api(question, 3)

    prompt_ = build_llm_prompt(question, rerank_results)

    pretty_json = json.dumps(prompt_, indent=4, ensure_ascii=False)

    # 打印格式化后的 JSON
    print(pretty_json)
